{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6","title":"Home"},{"location":"#welcome","text":"","title":"Welcome!"},{"location":"aws-land/","text":"AWS \u00b6 Networking \u00b6 EC2 instance bandwidth \u00b6 Amazon EC2 instance network bandwidth Instance bandwidth is only fully utilized within the same region. Misc \u00b6 Get EC2 instance metadata curl http://169.254.169.254/latest/meta-data/","title":"AWS"},{"location":"aws-land/#aws","text":"","title":"AWS"},{"location":"aws-land/#networking","text":"","title":"Networking"},{"location":"aws-land/#ec2-instance-bandwidth","text":"Amazon EC2 instance network bandwidth Instance bandwidth is only fully utilized within the same region.","title":"EC2 instance bandwidth"},{"location":"aws-land/#misc","text":"Get EC2 instance metadata curl http://169.254.169.254/latest/meta-data/","title":"Misc"},{"location":"cgroups/","text":"cgroups \u00b6 Ubuntu 18.04 \u00b6 view cgroup info of process via pid ps -o cgroup 12345 create memory cgroup sudo mkdir /sys/fs/cgroup/memory/cgroup-a limit memory of anything running in the cgroup cgroup-a to 4096 bytes echo 4096 | sudo tee /sys/fs/cgroup/memory/cgroup-a/memory.limit_in_bytes add process via pid to the cgroup cgroup-a sudo echo 12345 > /sys/fs/cgroup/memory/cgroup-a/cgroup.procs using libcgroup \u00b6 create memory cgroup cgroup-a sudo cgcreate -g memory:cgroup-a delete memory cgroup cgroup-a sudo cgdelete memory:cgroup-a run app app-a in cgroup cgroup-a sudo cgexec -g memory:cgroup-a app-a","title":"cgroups"},{"location":"cgroups/#cgroups","text":"","title":"cgroups"},{"location":"cgroups/#ubuntu-1804","text":"view cgroup info of process via pid ps -o cgroup 12345 create memory cgroup sudo mkdir /sys/fs/cgroup/memory/cgroup-a limit memory of anything running in the cgroup cgroup-a to 4096 bytes echo 4096 | sudo tee /sys/fs/cgroup/memory/cgroup-a/memory.limit_in_bytes add process via pid to the cgroup cgroup-a sudo echo 12345 > /sys/fs/cgroup/memory/cgroup-a/cgroup.procs","title":"Ubuntu 18.04"},{"location":"cgroups/#using-libcgroup","text":"create memory cgroup cgroup-a sudo cgcreate -g memory:cgroup-a delete memory cgroup cgroup-a sudo cgdelete memory:cgroup-a run app app-a in cgroup cgroup-a sudo cgexec -g memory:cgroup-a app-a","title":"using libcgroup"},{"location":"elasticsearch/","text":"Elasticsearch \u00b6 Data Node Sizing \u00b6 64-bit JVM \u00b6 Set Xms and Xmx to be the same and about 50% of the total physical memory allocated to node. Old GC stops threads from executing. To make sure GC runs fast enough it's good to keep the Xmx <= 30.5 GB. Node \u00b6 Shards \u00b6","title":"Elasticsearch"},{"location":"elasticsearch/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"elasticsearch/#data-node-sizing","text":"","title":"Data Node Sizing"},{"location":"elasticsearch/#64-bit-jvm","text":"Set Xms and Xmx to be the same and about 50% of the total physical memory allocated to node. Old GC stops threads from executing. To make sure GC runs fast enough it's good to keep the Xmx <= 30.5 GB.","title":"64-bit JVM"},{"location":"elasticsearch/#node","text":"","title":"Node"},{"location":"elasticsearch/#shards","text":"","title":"Shards"},{"location":"kubernetes/","text":"Kubernetes \u00b6 Cluster \u00b6 Host \u00b6 Critical OS system daemons placed in a top level cgroup. Scheduling \u00b6 Use pod priority to ensure system pods have scheduling priority. Kubelet \u00b6 Protect host from going OOM with --eviction-hard . It measures off of actual memory usage. Compared to memory requested. Pass Kubernetes system daemon cgroup in with --kube-reserved-cgroup . If no cgroup then reserve resources for the daemons with --kube-reserved . Pass OS system daemon cgroup in with --system-reserved-cgroup . If no cgroup then reserve resources for the daemons with --system-reserved . Place Kubernetes and OS system daemons in respective cgroups. Kubectl \u00b6 General Practices \u00b6 Use --context= to ensure correct context is targeted. CLI \u00b6 setup env sudo ln -s $(which kubectl) /usr/bin/k resource short-names svc = service ds = daemonset po = pod deploy = deployment vs = virtualservice gw = gateway ksvc = knative service quota = resourcequota get resource YAML k get <resource-type> <resoruce-id> -n <namespace> -o yaml get the pods from a Deployment deployment=<deployment name> k get pod --selector=\"$(k describe deployments $deployment | grep Selector | awk '{print $2}')\" --output=wide delete Service and Deployment deployment=<deployment-id> namespace=<namespace> k delete services $(k get svc --selector=$(k describe deployments $deployment -n $namespace | grep Selector | awk '{print $2}') | sed -n 2p | awk '{print $1}') k delete deploy $deployment -n $namespace view all node taints k get nodes -o json | jq '.items[] | .metadata.name, .spec.taints' create busybox pod for troubleshooting k run -i --tty --rm debug --image=busybox --restart=Never -- sh restart all pods in a deployment/daemonset k get pods -n namespace | grep pod-name | cut -d \" \" -f1 - | xargs k delete pod -n namespace remove the CRD finalizer blocking k patch crd/crontabs.stable.example.com -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge print resource usage in namespace k -n mynamespace describe resourcequotas seeing the latest jobs from a cronjob is as simple as listing all jobs in the namespace k -n mynamespace get jobs Reports \u00b6 see current resource usage vs limits per namespace k get quota -A -o custom-columns=NAMESPACE:metadata.namespace,CPU_LIMIT:{'.status.hard.limits\\.cpu'},CPU_USED:{'.status.used.limits\\.cpu'},MEM_LIMIT:{'.status.hard.limits\\.memory'},MEM_USED:{'.status.used.limits\\.memory'} Istio \u00b6 Verify mesh policy exists. k get policies.authentication.istio.io --all-namespaces k get meshpolicies.authentication.istio.io Get all istio destination rule hosts. k get destinationrules.networking.istio.io --all-namespaces -o yaml | grep \"host:\" Change envoy sidecare log level. k -n easybake exec -it -c istio-proxy easybake-ui-6bd7f9bf-9pb5w -- curl -XPOST http://localhost:15000/logging?level=trace Show given envoy configuration. istioctl proxy-config clusters -n istio-system istio-ingressgateway-65576f8745-kbvgl -o json Show routing for a port on a pod. istioctl proxy-config listeners easybake-ui-6bd7f9bf-klhvx -n easybake --port 3800 -o json Get configured proxy routes. useful for when traffic is not making it to destination. istioctl proxy-config route -n istio-system istio-ingressgateway-65576f8745-kbvgl -o json Traffic Management \u00b6 Enabled locality-load balancing if it's not enabled by default. Use outlier detection (OD) configurations in your destination rules to circuit break unhealthy nodes. Use locality-prioritized load balancing to decrease latency and egress costs. This mode tells envoy to send requests to pods on nodes that match the locality labels on the sender node. This mode does not work without OD configurations. General Networking \u00b6 Flush iptables on a Host \u00b6 systemctl stop docker.service iptables -F -t nat iptables -X -t nat iptables -F -t mangle iptables -X -t mangle iptables -F iptables -X systemctl start docker.service Weave \u00b6 Download the weave executable and place on k8s host. Make sure the version matches what is running in the cluster. Or shell into the weave container in the weave pod. Use the --local before all commands. Connection status between all hosts on the weave overlay network. If run on a healthy node the unhealthy node wont show up in the output. If run on an unhealthy node you will get no results ./weave status peers Connection from the host you are on to the other hosts in the k8s cluster on the weave overlay network. If run on a healthy node you will see connections that could not be established in the output if there is an unhealthy node. If run on an unhealthy node you will see all the connections fail. ./weave status connections Other resources: * weave kubernetes addon * troubleshooting weave Updating MTU \u00b6 To update the MTU edit the WEAVE_MTU env variable set for the weave container. After the patch the datapath interface on all nodes will get updated but the weave and VETH interfaces are not updated. Restarting the node will update the interfaces that were not updated. If all production endpoints in a single cluster I suggest scheduling this when end user activity is lowest. Debugging \u00b6 To enable debug logs edit the weave-net ds. Add the following env variable to the weave container. name: EXTRA_ARGS value: --log-level=debug Issues \u00b6 Disabling Fast Datapath (fastdp) Encryption \u00b6 Since the weave-net ds patching is 1 pod at a time non encrypted traffic is not accepted by nodes where the pod was not patched yet. Also this caused weave to overlay switch to sleeve. After the patch was fully complete restarting a pod would still result it no fastdp heartbeat ack (handleHeartbeatAck). I believe it's because of iptable rules that were not flushed/changed after the patch. After I flushed I saw established fastdp connections. Before the iptables flush I was seeing a ever increasing number of TX-DRP packets for the vxlan-6784 interface. Etcd \u00b6 The env variable $ETCDCTL_ENDPOINTS is set by default to the local listening address. For checking status and health interactively it's best to use the flag --write-out with the value \"fields\" , while it's easier to parse programmatically when the value is \"json\" . For the below I'll assume it's all interactive. Check health. etcdctl endpoint health --write-out=\"fields\" Check status. etcdctl endpoint status --write-out=\"fields\"","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"","title":"Kubernetes"},{"location":"kubernetes/#cluster","text":"","title":"Cluster"},{"location":"kubernetes/#host","text":"Critical OS system daemons placed in a top level cgroup.","title":"Host"},{"location":"kubernetes/#scheduling","text":"Use pod priority to ensure system pods have scheduling priority.","title":"Scheduling"},{"location":"kubernetes/#kubelet","text":"Protect host from going OOM with --eviction-hard . It measures off of actual memory usage. Compared to memory requested. Pass Kubernetes system daemon cgroup in with --kube-reserved-cgroup . If no cgroup then reserve resources for the daemons with --kube-reserved . Pass OS system daemon cgroup in with --system-reserved-cgroup . If no cgroup then reserve resources for the daemons with --system-reserved . Place Kubernetes and OS system daemons in respective cgroups.","title":"Kubelet"},{"location":"kubernetes/#kubectl","text":"","title":"Kubectl"},{"location":"kubernetes/#general-practices","text":"Use --context= to ensure correct context is targeted.","title":"General Practices"},{"location":"kubernetes/#cli","text":"setup env sudo ln -s $(which kubectl) /usr/bin/k resource short-names svc = service ds = daemonset po = pod deploy = deployment vs = virtualservice gw = gateway ksvc = knative service quota = resourcequota get resource YAML k get <resource-type> <resoruce-id> -n <namespace> -o yaml get the pods from a Deployment deployment=<deployment name> k get pod --selector=\"$(k describe deployments $deployment | grep Selector | awk '{print $2}')\" --output=wide delete Service and Deployment deployment=<deployment-id> namespace=<namespace> k delete services $(k get svc --selector=$(k describe deployments $deployment -n $namespace | grep Selector | awk '{print $2}') | sed -n 2p | awk '{print $1}') k delete deploy $deployment -n $namespace view all node taints k get nodes -o json | jq '.items[] | .metadata.name, .spec.taints' create busybox pod for troubleshooting k run -i --tty --rm debug --image=busybox --restart=Never -- sh restart all pods in a deployment/daemonset k get pods -n namespace | grep pod-name | cut -d \" \" -f1 - | xargs k delete pod -n namespace remove the CRD finalizer blocking k patch crd/crontabs.stable.example.com -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge print resource usage in namespace k -n mynamespace describe resourcequotas seeing the latest jobs from a cronjob is as simple as listing all jobs in the namespace k -n mynamespace get jobs","title":"CLI"},{"location":"kubernetes/#reports","text":"see current resource usage vs limits per namespace k get quota -A -o custom-columns=NAMESPACE:metadata.namespace,CPU_LIMIT:{'.status.hard.limits\\.cpu'},CPU_USED:{'.status.used.limits\\.cpu'},MEM_LIMIT:{'.status.hard.limits\\.memory'},MEM_USED:{'.status.used.limits\\.memory'}","title":"Reports"},{"location":"kubernetes/#istio","text":"Verify mesh policy exists. k get policies.authentication.istio.io --all-namespaces k get meshpolicies.authentication.istio.io Get all istio destination rule hosts. k get destinationrules.networking.istio.io --all-namespaces -o yaml | grep \"host:\" Change envoy sidecare log level. k -n easybake exec -it -c istio-proxy easybake-ui-6bd7f9bf-9pb5w -- curl -XPOST http://localhost:15000/logging?level=trace Show given envoy configuration. istioctl proxy-config clusters -n istio-system istio-ingressgateway-65576f8745-kbvgl -o json Show routing for a port on a pod. istioctl proxy-config listeners easybake-ui-6bd7f9bf-klhvx -n easybake --port 3800 -o json Get configured proxy routes. useful for when traffic is not making it to destination. istioctl proxy-config route -n istio-system istio-ingressgateway-65576f8745-kbvgl -o json","title":"Istio"},{"location":"kubernetes/#traffic-management","text":"Enabled locality-load balancing if it's not enabled by default. Use outlier detection (OD) configurations in your destination rules to circuit break unhealthy nodes. Use locality-prioritized load balancing to decrease latency and egress costs. This mode tells envoy to send requests to pods on nodes that match the locality labels on the sender node. This mode does not work without OD configurations.","title":"Traffic Management"},{"location":"kubernetes/#general-networking","text":"","title":"General Networking"},{"location":"kubernetes/#flush-iptables-on-a-host","text":"systemctl stop docker.service iptables -F -t nat iptables -X -t nat iptables -F -t mangle iptables -X -t mangle iptables -F iptables -X systemctl start docker.service","title":"Flush iptables on a Host"},{"location":"kubernetes/#weave","text":"Download the weave executable and place on k8s host. Make sure the version matches what is running in the cluster. Or shell into the weave container in the weave pod. Use the --local before all commands. Connection status between all hosts on the weave overlay network. If run on a healthy node the unhealthy node wont show up in the output. If run on an unhealthy node you will get no results ./weave status peers Connection from the host you are on to the other hosts in the k8s cluster on the weave overlay network. If run on a healthy node you will see connections that could not be established in the output if there is an unhealthy node. If run on an unhealthy node you will see all the connections fail. ./weave status connections Other resources: * weave kubernetes addon * troubleshooting weave","title":"Weave"},{"location":"kubernetes/#updating-mtu","text":"To update the MTU edit the WEAVE_MTU env variable set for the weave container. After the patch the datapath interface on all nodes will get updated but the weave and VETH interfaces are not updated. Restarting the node will update the interfaces that were not updated. If all production endpoints in a single cluster I suggest scheduling this when end user activity is lowest.","title":"Updating MTU"},{"location":"kubernetes/#debugging","text":"To enable debug logs edit the weave-net ds. Add the following env variable to the weave container. name: EXTRA_ARGS value: --log-level=debug","title":"Debugging"},{"location":"kubernetes/#issues","text":"","title":"Issues"},{"location":"kubernetes/#disabling-fast-datapath-fastdp-encryption","text":"Since the weave-net ds patching is 1 pod at a time non encrypted traffic is not accepted by nodes where the pod was not patched yet. Also this caused weave to overlay switch to sleeve. After the patch was fully complete restarting a pod would still result it no fastdp heartbeat ack (handleHeartbeatAck). I believe it's because of iptable rules that were not flushed/changed after the patch. After I flushed I saw established fastdp connections. Before the iptables flush I was seeing a ever increasing number of TX-DRP packets for the vxlan-6784 interface.","title":"Disabling Fast Datapath (fastdp) Encryption"},{"location":"kubernetes/#etcd","text":"The env variable $ETCDCTL_ENDPOINTS is set by default to the local listening address. For checking status and health interactively it's best to use the flag --write-out with the value \"fields\" , while it's easier to parse programmatically when the value is \"json\" . For the below I'll assume it's all interactive. Check health. etcdctl endpoint health --write-out=\"fields\" Check status. etcdctl endpoint status --write-out=\"fields\"","title":"Etcd"},{"location":"linux-system-calls/","text":"Linux System Calls \u00b6 seccomp (secure computing mode) \u00b6 Filter system calls issued by a program. The filters are based on BPF (Berkley Packet Filters). The idea behind seccomp is to restrict the system calls that can be made from a process, he said. The Linux kernel has a few hundred system calls, but most of them are not needed by any given process. If a process can be compromised and tricked into making other system calls, though, it may lead to a security vulnerability that could result in the compromise of the whole system. By restricting what system calls can be made, seccomp is a key component for building application sandboxes. #include <seccomp.h> /* libseccomp */ scmp_filter_ctx ctx; ctx = seccomp_init(SCMP_ACT_KILL); // default action: kill seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(dup2), 2, SCMP_A0(SCMP_CMP_EQ, 1), SCMP_A1(SCMP_CMP_EQ, 2)); // pass dup2(1,2); //fail dup2(2, 42); dup2 \u00b6 Change file descriptor. // redirect stderr to stdout dup2(1, 2); prlimit \u00b6 Get and set process resource limits. This routine makes ULIMIT(3) obsolete. struct rlimit old, new; struct rlimit *newp; pid_t pid; new.rlim_cur = /* soft limit */ new.rlim_max = /* hard limit */ newp = &new; prlimit(pid, RLIMIT_CPU, newp, &old) /* resource options in PRLIMIT(2) */ There is also a CLI tool for prlimit in PRLIMIT(1). printk \u00b6 Logging mechanism for debugging kernel space code. #include <linux/kernel.h> /* Needed for KERN_ALERT */ printk(\"<0>System dead.\\n\"); from userspace \u00b6 $ echo \"2Writing critical printk messages from userspace\" >/dev/kmsg $ dmesg console_loglevel \u00b6 To determine current console_loglevel: $ cat /proc/sys/kernel/printk The output values are in respect: current default minimum boot-time-default","title":"Linux System Calls"},{"location":"linux-system-calls/#linux-system-calls","text":"","title":"Linux System Calls"},{"location":"linux-system-calls/#seccomp-secure-computing-mode","text":"Filter system calls issued by a program. The filters are based on BPF (Berkley Packet Filters). The idea behind seccomp is to restrict the system calls that can be made from a process, he said. The Linux kernel has a few hundred system calls, but most of them are not needed by any given process. If a process can be compromised and tricked into making other system calls, though, it may lead to a security vulnerability that could result in the compromise of the whole system. By restricting what system calls can be made, seccomp is a key component for building application sandboxes. #include <seccomp.h> /* libseccomp */ scmp_filter_ctx ctx; ctx = seccomp_init(SCMP_ACT_KILL); // default action: kill seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(dup2), 2, SCMP_A0(SCMP_CMP_EQ, 1), SCMP_A1(SCMP_CMP_EQ, 2)); // pass dup2(1,2); //fail dup2(2, 42);","title":"seccomp (secure computing mode)"},{"location":"linux-system-calls/#dup2","text":"Change file descriptor. // redirect stderr to stdout dup2(1, 2);","title":"dup2"},{"location":"linux-system-calls/#prlimit","text":"Get and set process resource limits. This routine makes ULIMIT(3) obsolete. struct rlimit old, new; struct rlimit *newp; pid_t pid; new.rlim_cur = /* soft limit */ new.rlim_max = /* hard limit */ newp = &new; prlimit(pid, RLIMIT_CPU, newp, &old) /* resource options in PRLIMIT(2) */ There is also a CLI tool for prlimit in PRLIMIT(1).","title":"prlimit"},{"location":"linux-system-calls/#printk","text":"Logging mechanism for debugging kernel space code. #include <linux/kernel.h> /* Needed for KERN_ALERT */ printk(\"<0>System dead.\\n\");","title":"printk"},{"location":"linux-system-calls/#from-userspace","text":"$ echo \"2Writing critical printk messages from userspace\" >/dev/kmsg $ dmesg","title":"from userspace"},{"location":"linux-system-calls/#console_loglevel","text":"To determine current console_loglevel: $ cat /proc/sys/kernel/printk The output values are in respect: current default minimum boot-time-default","title":"console_loglevel"},{"location":"vault/","text":"Vault \u00b6 Decrypt a single secret \u00b6 echo '$ANSIBLE_VAULT;1.1;AES256 62303333633932333833363837376365366465636466373433326630663634376461666135633039 6266383335653333616263316230326164353632383161310a306437653362383436643565336663 62643037376439306431343935656132643964343766653465373933363262376565353561326231 3432343937313639630a343131303535633737313563623337636163383739393933373663396665 62323734353634356462326237373931623735663739323064393133386366333230' | ansible-vault decrypt /dev/stdin --output=/dev/stderr > /dev/null","title":"HashiCorp Vault"},{"location":"vault/#vault","text":"","title":"Vault"},{"location":"vault/#decrypt-a-single-secret","text":"echo '$ANSIBLE_VAULT;1.1;AES256 62303333633932333833363837376365366465636466373433326630663634376461666135633039 6266383335653333616263316230326164353632383161310a306437653362383436643565336663 62643037376439306431343935656132643964343766653465373933363262376565353561326231 3432343937313639630a343131303535633737313563623337636163383739393933373663396665 62323734353634356462326237373931623735663739323064393133386366333230' | ansible-vault decrypt /dev/stdin --output=/dev/stderr > /dev/null","title":"Decrypt a single secret"}]}